{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d52b1d-5a87-491b-9113-859d11574c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import cm\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.ndimage import label, find_objects, binary_erosion, binary_dilation, distance_transform_edt\n",
    "from scipy import stats as scipy_stats\n",
    "from scipy.signal import find_peaks, savgol_filter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial import Delaunay, distance\n",
    "from sklearn.cluster import DBSCAN\n",
    "from skimage import filters\n",
    "from skimage.filters import threshold_otsu, threshold_multiotsu\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import fsolve\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf3bab-18d3-4128-8d6b-c2904870fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder structure\n",
    "input_dir = 'input_data'\n",
    "output_dir = 'output_data'\n",
    "\n",
    "# Input staleite data, i.e.: https://browser.dataspace.copernicus.eu/\n",
    "optical_image_path = os.path.join(input_dir, 'Gdansk_2025-04-10-00_00_2025-04-10-23_59_Sentinel-2_Quarterly_Mosaics_True_Color_Cloudless.jpg')\n",
    "sar_image_path = os.path.join(input_dir, 'Gdansk_2025-04-07-00_00_2025-04-07-23_59_Sentinel-1_IW_VV+VH_VV_-_decibel_gamma0.jpg')\n",
    "\n",
    "# Extract base name for input files (without extension)\n",
    "base_name = os.path.splitext(os.path.basename(optical_image_path))[0].split('_optical')[0]\n",
    "\n",
    "# Define resolution: i.e. 20m per pixel\n",
    "satellite_pixel_resolution = 15\n",
    "\n",
    "# Set ticks intervals for different plots\n",
    "image_x_tick_interval = 5000  # meters (5km)\n",
    "image_y_tick_interval = image_x_tick_interval  # meters (5km)\n",
    "gradient_x_tick_interval = 1000  # meters (1km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c54f6f5-6314-4052-82e9-44b9bb08925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose color histogram tool\n",
    "def decompose_histogram(image, num_peaks=3, value_range=(0, 2), num_bins=50, exclude_zeros=False):\n",
    "    \"\"\"\n",
    "    Decompose an image histogram into a specified number of Gaussian components.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : 2D numpy array\n",
    "        The input image to be analyzed\n",
    "    num_peaks : int\n",
    "        Number of Gaussian peaks to fit (default: 3)\n",
    "    value_range : tuple\n",
    "        Range of values to consider for the histogram (default: (0, 2))\n",
    "    num_bins : int\n",
    "        Number of bins for the histogram (default: 50)\n",
    "    exclude_zeros : bool\n",
    "        Whether to exclude zero values from the analysis (default: False)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing the fitted parameters and component data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flatten the image data\n",
    "    flat_data = image.flatten()\n",
    "    \n",
    "    # Exclude zeros if requested\n",
    "    if exclude_zeros:\n",
    "        flat_data = flat_data[flat_data != 0]\n",
    "        \n",
    "        # Check if we have any data left after excluding zeros\n",
    "        if len(flat_data) == 0:\n",
    "            print(\"Warning: No non-zero data points found. Cannot perform analysis.\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"Excluded zeros. Analyzing {len(flat_data)} non-zero values.\")\n",
    "    \n",
    "    # Create histogram data\n",
    "    hist, bin_edges = np.histogram(flat_data, bins=num_bins, range=value_range)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    # Define a function for multiple Gaussian components\n",
    "    def multi_gaussian(x, *params):\n",
    "        \"\"\"\n",
    "        Sum of multiple Gaussian distributions.\n",
    "        params format: [a1, mu1, sigma1, a2, mu2, sigma2, ...]\n",
    "        \"\"\"\n",
    "        y = np.zeros_like(x)\n",
    "        for i in range(0, len(params), 3):\n",
    "            a = params[i]\n",
    "            mu = params[i+1]\n",
    "            sigma = params[i+2]\n",
    "            y += a * np.exp(-(x - mu)**2 / (2 * sigma**2))\n",
    "        return y\n",
    "    \n",
    "    # Define a single Gaussian function for intersection analysis\n",
    "    def single_gaussian(x, a, mu, sigma):\n",
    "        \"\"\"Single Gaussian distribution.\"\"\"\n",
    "        return a * np.exp(-(x - mu)**2 / (2 * sigma**2))\n",
    "    \n",
    "    # Create initial guess and bounds for parameters\n",
    "    initial_guess = []\n",
    "    lower_bounds = []\n",
    "    upper_bounds = []\n",
    "    \n",
    "    max_amplitude = np.max(hist)\n",
    "    value_min, value_max = value_range\n",
    "    range_width = value_max - value_min\n",
    "    \n",
    "    # Get peak indices from histogram to improve initial guesses\n",
    "    # Find local maxima in the histogram\n",
    "    from scipy.signal import find_peaks\n",
    "    peak_indices, _ = find_peaks(hist, height=max(hist) * 0.2)\n",
    "    \n",
    "    # If we found some peaks, use them for initial guesses\n",
    "    if len(peak_indices) > 0 and len(peak_indices) <= num_peaks:\n",
    "        peak_positions = bin_centers[peak_indices]\n",
    "        peak_heights = hist[peak_indices]\n",
    "        \n",
    "        # Sort by height, descending\n",
    "        sorted_indices = np.argsort(peak_heights)[::-1]\n",
    "        peak_positions = peak_positions[sorted_indices]\n",
    "        peak_heights = peak_heights[sorted_indices]\n",
    "        \n",
    "        # Use detected peaks first, then add evenly spaced ones if needed\n",
    "        for i in range(num_peaks):\n",
    "            if i < len(peak_positions):\n",
    "                # Use detected peak\n",
    "                amplitude = peak_heights[i]\n",
    "                mean = peak_positions[i]\n",
    "            else:\n",
    "                # Add evenly spaced peaks for remaining\n",
    "                amplitude = max_amplitude / (num_peaks * 2)\n",
    "                mean = value_min + range_width * (i + 0.5) / num_peaks\n",
    "            \n",
    "            # Amplitude\n",
    "            initial_guess.append(amplitude)\n",
    "            lower_bounds.append(0)\n",
    "            upper_bounds.append(np.inf)\n",
    "            \n",
    "            # Mean\n",
    "            initial_guess.append(mean)\n",
    "            lower_bounds.append(value_min)\n",
    "            upper_bounds.append(value_max)\n",
    "            \n",
    "            # Standard deviation - wider initial guess for better convergence\n",
    "            initial_guess.append(range_width / (4 * num_peaks))\n",
    "            lower_bounds.append(range_width / (50 * num_peaks))  # Allow narrower peaks\n",
    "            upper_bounds.append(range_width)  # Allow wider peaks\n",
    "    else:\n",
    "        # Fall back to evenly spaced means if peak detection fails\n",
    "        for i in range(num_peaks):\n",
    "            # Amplitude\n",
    "            initial_guess.append(max_amplitude / num_peaks)\n",
    "            lower_bounds.append(0)\n",
    "            upper_bounds.append(np.inf)\n",
    "            \n",
    "            # Mean (evenly distribute initial guesses across the range)\n",
    "            mean_guess = value_min + range_width * (i + 0.5) / num_peaks\n",
    "            initial_guess.append(mean_guess)\n",
    "            lower_bounds.append(value_min)\n",
    "            upper_bounds.append(value_max)\n",
    "            \n",
    "            # Standard deviation - wider initial guess for better convergence\n",
    "            initial_guess.append(range_width / (4 * num_peaks))\n",
    "            lower_bounds.append(range_width / (50 * num_peaks))  # Allow narrower peaks\n",
    "            upper_bounds.append(range_width)  # Allow wider peaks\n",
    "    \n",
    "    # Perform the curve fit with increased max iterations and tweaked optimization parameters\n",
    "    try:\n",
    "        params, covariance = curve_fit(\n",
    "            multi_gaussian, \n",
    "            bin_centers, \n",
    "            hist, \n",
    "            p0=initial_guess,\n",
    "            bounds=(lower_bounds, upper_bounds),\n",
    "            maxfev=10000,  # Increase maximum function evaluations\n",
    "            method='trf',  # Use Trust Region Reflective algorithm\n",
    "            ftol=1e-6,     # Function tolerance for termination\n",
    "            xtol=1e-6      # Parameter tolerance for termination\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Fitting error: {e}\")\n",
    "        print(\"Trying alternative optimization approach...\")\n",
    "        \n",
    "        # Try a simpler approach: reduce constraints and use different method\n",
    "        try:\n",
    "            # Remove bounds and use different method\n",
    "            params, covariance = curve_fit(\n",
    "                multi_gaussian, \n",
    "                bin_centers, \n",
    "                hist, \n",
    "                p0=initial_guess,\n",
    "                maxfev=20000,      # More iterations\n",
    "                method='lm'        # Levenberg-Marquardt algorithm (no bounds but can be more robust)\n",
    "            )\n",
    "            print(\"Alternative fitting approach succeeded!\")\n",
    "        except RuntimeError as e2:\n",
    "            print(f\"Alternative fitting also failed: {e2}\")\n",
    "            return None\n",
    "    \n",
    "    # Calculate the fitted values\n",
    "    fitted_y = multi_gaussian(bin_centers, *params)\n",
    "    \n",
    "    # Calculate individual Gaussian components\n",
    "    components = []\n",
    "    for i in range(0, len(params), 3):\n",
    "        a = params[i]\n",
    "        mu = params[i+1]\n",
    "        sigma = params[i+2]\n",
    "        component = a * np.exp(-(bin_centers - mu)**2 / (2 * sigma**2))\n",
    "        components.append(component)\n",
    "    \n",
    "    # Calculate the contribution of each component\n",
    "    total_area = np.sum(fitted_y)\n",
    "    contributions = [np.sum(comp) / total_area * 100 for comp in components]\n",
    "    \n",
    "    # Calculate intersections between adjacent Gaussian components\n",
    "    intersections = []\n",
    "    \n",
    "    # Function to find intersection point between two Gaussian curves\n",
    "    def find_intersection(params1, params2, x_guess):\n",
    "        a1, mu1, sigma1 = params1\n",
    "        a2, mu2, sigma2 = params2\n",
    "        \n",
    "        def func(x):\n",
    "            return single_gaussian(x, a1, mu1, sigma1) - single_gaussian(x, a2, mu2, sigma2)\n",
    "        \n",
    "        try:\n",
    "            result = fsolve(func, x_guess)\n",
    "            # Check if the solution is within our range\n",
    "            if value_min <= result[0] <= value_max:\n",
    "                # Verify it's actually an intersection\n",
    "                y1 = single_gaussian(result[0], a1, mu1, sigma1)\n",
    "                y2 = single_gaussian(result[0], a2, mu2, sigma2)\n",
    "                if np.isclose(y1, y2, rtol=1e-3) and y1 > 0:\n",
    "                    return result[0], y1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return None, None\n",
    "    \n",
    "    # Sort the components by their means\n",
    "    component_indices = list(range(num_peaks))\n",
    "    component_indices.sort(key=lambda i: params[i*3+1])\n",
    "    \n",
    "    intersection_data = []\n",
    "    \n",
    "    # Find intersections between adjacent components\n",
    "    for i in range(len(component_indices)-1):\n",
    "        idx1 = component_indices[i]\n",
    "        idx2 = component_indices[i+1]\n",
    "        \n",
    "        # Extract parameters for each component\n",
    "        params1 = params[idx1*3:idx1*3+3]\n",
    "        params2 = params[idx2*3:idx2*3+3]\n",
    "        \n",
    "        # Initial guess for intersection: midpoint between means\n",
    "        x_guess = (params1[1] + params2[1]) / 2\n",
    "        \n",
    "        # Find intersection\n",
    "        x_intersect, y_intersect = find_intersection(params1, params2, x_guess)\n",
    "        \n",
    "        if x_intersect is not None:\n",
    "            intersections.append((x_intersect, y_intersect))\n",
    "            intersection_data.append({\n",
    "                'components': (idx1+1, idx2+1),\n",
    "                'x_value': x_intersect,\n",
    "                'y_value': y_intersect,\n",
    "                'relative_height': y_intersect / max(np.max(components[idx1]), np.max(components[idx2]))\n",
    "            })\n",
    "    \n",
    "    # Create a summary plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot original histogram envelope\n",
    "    plt.plot(bin_centers, hist, 'ko', label='Original Data', alpha=0.5)\n",
    "    \n",
    "    # Plot the combined fit\n",
    "    plt.plot(bin_centers, fitted_y, 'r-', linewidth=2, label='Combined Fit')\n",
    "    \n",
    "    # Plot individual Gaussian components\n",
    "    colors = ['g', 'b', 'm', 'c', 'y', 'orange', 'purple', 'brown', 'pink', 'gray']\n",
    "    component_params = []\n",
    "    \n",
    "    for i in range(num_peaks):\n",
    "        idx = i * 3\n",
    "        a = params[idx]\n",
    "        mu = params[idx+1]\n",
    "        sigma = params[idx+2]\n",
    "        contrib = contributions[i]\n",
    "        \n",
    "        color = colors[i % len(colors)]\n",
    "        plt.plot(bin_centers, components[i], f'{color}--', linewidth=1.5, \n",
    "                 label=f'Gaussian {i+1}: μ={mu:.2f}, σ={sigma:.2f}, {contrib:.1f}%')\n",
    "        \n",
    "        component_params.append({\n",
    "            'amplitude': a,\n",
    "            'mean': mu,\n",
    "            'std_dev': sigma,\n",
    "            'contribution': contrib\n",
    "        })\n",
    "    \n",
    "    # Mark intersection points\n",
    "    for x, y in intersections:\n",
    "        plt.plot(x, y, 'ro', markersize=8)\n",
    "        plt.axvline(x=x, color='k', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Pixel Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    if exclude_zeros:\n",
    "        plt.title(f'Decomposition of Histogram into {num_peaks} Gaussian Components (Zeros Excluded)')\n",
    "    else:\n",
    "        plt.title(f'Decomposition of Histogram into {num_peaks} Gaussian Components')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the fitted parameters\n",
    "    print(f\"Fitted Parameters for {num_peaks} Gaussian Components:\")\n",
    "    for i, comp in enumerate(component_params):\n",
    "        print(f\"Gaussian {i+1}: Amplitude={comp['amplitude']:.1f}, \"\n",
    "              f\"Mean={comp['mean']:.3f}, StdDev={comp['std_dev']:.3f}, \"\n",
    "              f\"Contribution={comp['contribution']:.1f}%\")\n",
    "    \n",
    "    # Print intersection information\n",
    "    if intersection_data:\n",
    "        print(\"\\nIntersection Points Between Gaussian Components:\")\n",
    "        for i, idata in enumerate(intersection_data):\n",
    "            print(f\"Intersection {i+1}: Between Gaussian {idata['components'][0]} and {idata['components'][1]}\")\n",
    "            print(f\"  Position: x={idata['x_value']:.3f}, y={idata['y_value']:.1f}\")\n",
    "            print(f\"  Relative height: {idata['relative_height']*100:.1f}% of max component height\")\n",
    "    else:\n",
    "        print(\"\\nNo valid intersections found between Gaussian components.\")\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'bin_centers': bin_centers,\n",
    "        'histogram': hist,\n",
    "        'fitted_curve': fitted_y,\n",
    "        'components': components,\n",
    "        'component_params': component_params,\n",
    "        'raw_params': params,\n",
    "        'intersections': intersection_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f424895-1bb0-47d3-a60b-27dc780aea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process optical image\n",
    "optical_image = cv2.imread(optical_image_path)\n",
    "optical_image_rgb = cv2.cvtColor(optical_image, cv2.COLOR_BGR2RGB)\n",
    "optical_image_gray = cv2.cvtColor(optical_image, cv2.COLOR_BGR2GRAY)  # Simplified conversion\n",
    "\n",
    "# Edge detection with Sobel\n",
    "optical_image_sobelx = cv2.Sobel(optical_image_gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "optical_image_sobely = cv2.Sobel(optical_image_gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "optical_image_sobel_combined = cv2.magnitude(optical_image_sobelx, optical_image_sobely)\n",
    "optical_image_sobel_combined = cv2.normalize(optical_image_sobel_combined, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "\n",
    "# Create edge density map\n",
    "kernel_size = 15\n",
    "optical_image_density_map = cv2.GaussianBlur(optical_image_sobel_combined, (kernel_size, kernel_size), 0)\n",
    "# Fix: Convert to float before normalizing\n",
    "optical_image_density_map = optical_image_density_map.astype(float) / np.max(optical_image_density_map)\n",
    "\n",
    "# Load and normalize SAR image\n",
    "sar_image = cv2.imread(sar_image_path, cv2.IMREAD_GRAYSCALE).astype(float) / 255.0\n",
    "\n",
    "# Combine images and apply Non-Local Means smoothing\n",
    "combined_image = optical_image_density_map + sar_image\n",
    "combined_image_uint8 = np.clip(combined_image * 127.5, 0, 255).astype(np.uint8)\n",
    "smoothed_image_uint8 = cv2.fastNlMeansDenoising(combined_image_uint8, None, 10, 7, 21)\n",
    "combined_image = smoothed_image_uint8.astype(float) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb395c-7f3d-4417-9ca9-f859c45b6bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define water_threshold and urban_threshold based of histograms decomposition\n",
    "result = decompose_histogram(combined_image, num_peaks=3, exclude_zeros=False)\n",
    "intersection_x_values = []\n",
    "if result and 'intersections' in result:\n",
    "    for intersection in result['intersections']:\n",
    "        intersection_x_values.append(intersection['x_value'])\n",
    "\n",
    "# Define thresholds for segmentation\n",
    "#water_threshold = intersection_x_values[0]\n",
    "#urban_threshold = intersection_x_values[1]\n",
    "water_threshold = 0.2\n",
    "urban_threshold = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36332bb-47f5-475d-b241-8162a49bad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial segmentation\n",
    "segmentation = np.zeros_like(combined_image, dtype=np.uint8)\n",
    "segmentation[(combined_image < water_threshold)] = 1  # Water\n",
    "segmentation[(combined_image >= water_threshold) & (combined_image < urban_threshold)] = 2  # Terrain\n",
    "segmentation[(combined_image >= urban_threshold)] = 3  # Urban\n",
    "\n",
    "# Process urban mask\n",
    "urban_mask = (segmentation == 3).astype(np.uint8) * 255\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "urban_mask_closed = cv2.morphologyEx(\n",
    "    cv2.dilate(urban_mask, kernel, iterations=1), \n",
    "    cv2.MORPH_CLOSE, \n",
    "    kernel\n",
    ")\n",
    "\n",
    "# Filter small urban patches\n",
    "num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(urban_mask_closed, connectivity=8)\n",
    "urban_mask_filtered = np.zeros_like(urban_mask_closed)\n",
    "for i in range(1, num_labels):\n",
    "    if stats[i, cv2.CC_STAT_AREA] >= 100:  # min_urban_area = 100\n",
    "        urban_mask_filtered[labels == i] = 255\n",
    "\n",
    "# Create final segmentation\n",
    "improved_segmentation = segmentation.copy()\n",
    "improved_segmentation[improved_segmentation == 3] = 2  # Reset urban to terrain\n",
    "improved_segmentation[urban_mask_filtered == 255] = 3  # Set filtered urban areas\n",
    "\n",
    "# Apply colormap for visualization\n",
    "colormap = np.array([\n",
    "    [0, 0, 0],      # Background\n",
    "    [0, 0, 255],    # Water (blue)\n",
    "    [0, 255, 0],    # Terrain (green)\n",
    "    [255, 0, 0]     # Urban (red)\n",
    "], dtype=np.uint8)\n",
    "segmented_image = colormap[improved_segmentation]\n",
    "\n",
    "# Create masks for density analysis\n",
    "urban_mask = improved_segmentation == 3\n",
    "urban_mask_numeric = urban_mask.astype(float)\n",
    "urban_density = combined_image * urban_mask_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef5b821-6adf-44ef-b9b0-d7b265e83ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define urban_center_threshold based of histograms decomposition\n",
    "result = decompose_histogram(urban_density, num_peaks=2, exclude_zeros=True)\n",
    "intersection_x_values = []\n",
    "if result and 'intersections' in result:\n",
    "    for intersection in result['intersections']:\n",
    "        intersection_x_values.append(intersection['x_value'])\n",
    "\n",
    "# Define thresholds for urban center segmentation\n",
    "urban_center_threshold = intersection_x_values[0]\n",
    "urban_centers_mask = (combined_image > urban_center_threshold) & urban_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d874f6-3a98-42f1-89d1-ec683931f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to set axes in meters\n",
    "def set_axes_in_meters(ax, img_shape):\n",
    "    height, width = img_shape[:2]\n",
    "    # Create x and y axes in meters\n",
    "    x_meters = np.arange(0, width * satellite_pixel_resolution, satellite_pixel_resolution)\n",
    "    y_meters = np.arange(0, height * satellite_pixel_resolution, satellite_pixel_resolution)\n",
    "    \n",
    "    # Set ticks at regular intervals (5km)\n",
    "    image_x_tick_interval = 5000  # meters (5km)\n",
    "    image_y_tick_interval = 5000  # meters (5km)\n",
    "    \n",
    "    x_ticks = np.arange(0, width * satellite_pixel_resolution, image_x_tick_interval)\n",
    "    y_ticks = np.arange(0, height * satellite_pixel_resolution, image_y_tick_interval)\n",
    "    \n",
    "    ax.set_xticks(x_ticks / satellite_pixel_resolution)\n",
    "    ax.set_yticks(y_ticks / satellite_pixel_resolution)\n",
    "    ax.set_xticklabels([f'{int(x/1000)}km' for x in x_ticks])\n",
    "    ax.set_yticklabels([f'{int(y/1000)}km' for y in y_ticks])\n",
    "    \n",
    "    ax.set_xlabel('Distance (km)')\n",
    "    ax.set_ylabel('Distance (km)')\n",
    "\n",
    "# Individual Plots - saves each image as a separate file\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "# 1. Original Image\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "plt.imshow(optical_image_rgb)\n",
    "plt.title('Original Optical Image')\n",
    "set_axes_in_meters(ax, optical_image_rgb.shape)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/{base_name}_original.png', dpi=300, bbox_inches='tight')\n",
    "print(f'{output_dir}/{base_name}_original.png file saved')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# 2. Sobel Edge Detection\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "plt.imshow(optical_image_sobel_combined, cmap='gray')\n",
    "plt.title('Sobel Edge Detection')\n",
    "set_axes_in_meters(ax, optical_image_sobel_combined.shape)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/{base_name}_sobel.png', dpi=300, bbox_inches='tight')\n",
    "print(f'{output_dir}/{base_name}_sobel.png file saved')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# 3. Edge Density Map\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "plt.imshow(optical_image_density_map, cmap='gray')\n",
    "plt.title('Edge Density Map (0-1 range)')\n",
    "set_axes_in_meters(ax, optical_image_density_map.shape)\n",
    "# Add colorbar with fixed size\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(ax.images[0], cax=cax)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/{base_name}_density.png', dpi=300, bbox_inches='tight')\n",
    "print(f'{output_dir}/{base_name}_density.png file saved')\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n",
    "# 4. SAR Image\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "plt.imshow(sar_image, cmap='gray')\n",
    "plt.title('SAR Image (0-1 range)')\n",
    "set_axes_in_meters(ax, sar_image.shape)\n",
    "# Add colorbar with fixed size\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(ax.images[0], cax=cax)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/{base_name}_sar.png', dpi=300, bbox_inches='tight')\n",
    "print(f'{output_dir}/{base_name}_sar.png file saved')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# 5. Combined Result\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "im = plt.imshow(combined_image, cmap='viridis')\n",
    "plt.title('Combined Result (0-2 range)')\n",
    "set_axes_in_meters(ax, combined_image.shape)\n",
    "# Add colorbar with fixed size\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(im, cax=cax)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/{base_name}_combined.png', dpi=300, bbox_inches='tight')\n",
    "print(f'{output_dir}/{base_name}_combined.png file saved')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# 6. Segmented Image\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "plt.imshow(segmented_image)  # No cmap needed - it's already an RGB image\n",
    "plt.title('Improved Segmentation')\n",
    "set_axes_in_meters(ax, segmented_image.shape)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/{base_name}_segmented.png', dpi=300, bbox_inches='tight')\n",
    "print(f'{output_dir}/{base_name}_segmented.png file saved')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# 7. Urban Density Map\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "im = plt.imshow(urban_density, cmap='viridis')\n",
    "plt.title('Urban Density Map (0-2 range)')\n",
    "set_axes_in_meters(ax, urban_density.shape)\n",
    "# Add colorbar with fixed size\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(im, cax=cax)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/{base_name}_urban_density.png', dpi=300, bbox_inches='tight')\n",
    "print(f'{output_dir}/{base_name}_urban_density.png file saved')\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n",
    "# 8. Urban Centers\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.gca()\n",
    "plt.imshow(urban_mask, cmap='gray', alpha=0.5)\n",
    "plt.imshow(urban_centers_mask, cmap='Reds', alpha=0.8)\n",
    "plt.title(f\"Urban Centers (Density > {urban_center_threshold})\")\n",
    "set_axes_in_meters(ax, urban_centers_mask.shape)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/{base_name}_urban_centers.png', dpi=300, bbox_inches='tight')\n",
    "print(f'{output_dir}/{base_name}_urban_centers.png file saved')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1111b8ec-1d25-450f-8881-5fa16e3088d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set gradient_intercept to the minimum of the urban density\n",
    "gradient_intercept = urban_density.min()\n",
    "\n",
    "# Calculate density gradients from centers\n",
    "def calculate_density_gradients(centers_mask, urban_mask, density_map):\n",
    "    \"\"\"Calculate how density changes with distance from urban centers\"\"\"\n",
    "    # Calculate distance from each urban pixel to the nearest center\n",
    "    # First invert the centers mask to get distance transform\n",
    "    centers_dist_transform = distance_transform_edt(~centers_mask)\n",
    "    \n",
    "    # Only consider distances within urban areas\n",
    "    urban_distances = centers_dist_transform[urban_mask]\n",
    "    urban_densities = density_map[urban_mask]\n",
    "    \n",
    "    # Bin distances to create gradient profile\n",
    "    max_dist = int(np.max(urban_distances)) + 1\n",
    "    distance_bins = range(max_dist)\n",
    "    density_by_distance = []\n",
    "    \n",
    "    for d in distance_bins:\n",
    "        # Get densities at this distance\n",
    "        mask = (urban_distances >= d) & (urban_distances < d+1)\n",
    "        if np.sum(mask) > 0:\n",
    "            avg_density = np.mean(urban_densities[mask])\n",
    "            density_by_distance.append((d, avg_density))\n",
    "    \n",
    "    return density_by_distance\n",
    "\n",
    "# Calculate density gradients\n",
    "density_gradients = calculate_density_gradients(urban_centers_mask, urban_mask, combined_image)\n",
    "\n",
    "# Density gradient plot data\n",
    "distances = [d for d, _ in density_gradients]\n",
    "densities = [den for _, den in density_gradients]\n",
    "\n",
    "# Convert to numpy arrays for analysis\n",
    "distances_np = np.array(distances)\n",
    "densities_np = np.array(densities)\n",
    "\n",
    "# Convert pixel distances to kilometers using satellite resolution\n",
    "distances_km = distances_np * satellite_pixel_resolution / 1000  # Divide by 1000 to convert m to km\n",
    "\n",
    "# Create a single figure with the density gradient plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot the original density gradient\n",
    "plt.plot(distances_km, densities_np, 'bo-', linewidth=2, label='Density Gradient')\n",
    "\n",
    "# Find local centers (bumps/peaks)\n",
    "if len(distances_np) > 5:\n",
    "    prominence = 0.02 * (max(densities_np) - min(densities_np))\n",
    "    peaks, _ = find_peaks(densities_np, prominence=prominence)\n",
    "    \n",
    "    # Find local minima (valleys)\n",
    "    neg_densities = -1 * densities_np\n",
    "    minima, _ = find_peaks(neg_densities, prominence=prominence)\n",
    "else:\n",
    "    peaks = []\n",
    "    minima = []\n",
    "\n",
    "# Skip the first point - don't add it to minima\n",
    "# Add only last point if needed\n",
    "if (len(distances_np)-1) not in peaks and (len(distances_np)-1) not in minima:\n",
    "    minima = np.append(minima, np.array([len(distances_np)-1]))\n",
    "\n",
    "# Sort minima to ensure they're in ascending order\n",
    "minima = np.sort(minima)\n",
    "\n",
    "# Calculate linear regression through minima\n",
    "if len(minima) > 1:\n",
    "    # Use scipy_stats to avoid conflicts with any existing 'stats' variable\n",
    "    slope_pixel, intercept, r_value, p_value, std_err = scipy_stats.linregress(\n",
    "        distances_np[minima], densities_np[minima])\n",
    "    \n",
    "    # Calculate slope in terms of km\n",
    "    slope_km = slope_pixel * 1000 / satellite_pixel_resolution  # Convert to per-km\n",
    "    \n",
    "    # Create trendline for all points\n",
    "    trendline = slope_pixel * distances_np + intercept\n",
    "    \n",
    "    # Define the y-value for which to calculate the intercept\n",
    "    target_y_value = gradient_intercept  # Example value - adjust as needed\n",
    "    \n",
    "    # Calculate x-value where the line intersects target_y_value\n",
    "    if abs(slope_pixel) > 1e-10:  # Avoid division by near-zero\n",
    "        x_pixels_at_target_y = (target_y_value - intercept) / slope_pixel\n",
    "        ld_distance_min_km = x_pixels_at_target_y * satellite_pixel_resolution / 1000  # Convert to km\n",
    "    else:\n",
    "        x_pixels_at_target_y = float('inf')  # If slope is essentially zero\n",
    "        ld_distance_min_km = float('inf')\n",
    "    \n",
    "    # Plot trendline\n",
    "    plt.plot(distances_km, trendline, 'g--', linewidth=2, \n",
    "             label=f'Alfa: {slope_km:.4f}/km\\nDistance: {ld_distance_min_km:.2f} km')\n",
    "    \n",
    "    # Calculate MSE between actual and trendline\n",
    "    mse = mean_squared_error(densities_np, trendline)\n",
    "else:\n",
    "    mse = 0\n",
    "    ld_distance_min_km = float('inf')\n",
    "\n",
    "# Add horizontal line at center threshold\n",
    "plt.axhline(y=urban_center_threshold, color='r', linestyle='--', alpha=0.7, label=f\"Centers Threshold ({urban_center_threshold})\")\n",
    "\n",
    "# Add target density to legend without drawing a line\n",
    "plt.plot([], [], ' ', label=f\"Target Density ({target_y_value:.2f})\")\n",
    "\n",
    "# If the intercept is within the plot range, mark it\n",
    "if 0 <= ld_distance_min_km <= max(distances_km):\n",
    "    plt.axvline(x=ld_distance_min_km, color='m', linestyle=':', alpha=0.5)\n",
    "    plt.plot(ld_distance_min_km, target_y_value, 'mo', markersize=8)\n",
    "\n",
    "# Set x-axis ticks using the defined interval\n",
    "max_distance_km = np.ceil(max(distances_km))\n",
    "x_ticks = np.arange(0, max_distance_km + 1, gradient_x_tick_interval/1000)  # Convert m to km\n",
    "plt.xticks(x_ticks)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.xlabel('Distance from Urban Center (km)')\n",
    "plt.ylabel('Average Density')\n",
    "plt.title(f'Density Gradient Through Minima (MSE: {mse:.6f})')\n",
    "plt.legend()\n",
    "\n",
    "# Make the plot have good proportions\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/{base_name}_urban_density_gradient.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7261558c-700d-4d2d-9516-43bda9438d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate density gradients from centers\n",
    "def calculate_enhanced_density_gradients(centers_mask, urban_mask, density_map):\n",
    "    \"\"\"Calculate how density changes with distance from urban centers with enhanced statistics\"\"\"\n",
    "    # Calculate distance from each urban pixel to the nearest center\n",
    "    centers_dist_transform = distance_transform_edt(~centers_mask)\n",
    "    \n",
    "    # Only consider distances within urban areas\n",
    "    urban_distances = centers_dist_transform[urban_mask]\n",
    "    urban_densities = density_map[urban_mask]\n",
    "    \n",
    "    # Bin distances to create gradient profile with enhanced statistics\n",
    "    max_dist = int(np.max(urban_distances)) + 1\n",
    "    distance_bins = range(max_dist)\n",
    "    density_stats_by_distance = []\n",
    "    \n",
    "    for d in distance_bins:\n",
    "        # Get densities at this distance\n",
    "        mask = (urban_distances >= d) & (urban_distances < d+1)\n",
    "        if np.sum(mask) > 0:\n",
    "            densities_at_d = urban_densities[mask]\n",
    "            stats = {\n",
    "                \"distance\": d,\n",
    "                \"min\": np.min(densities_at_d),\n",
    "                \"q1\": np.percentile(densities_at_d, 25),\n",
    "                \"median\": np.median(densities_at_d),\n",
    "                \"mean\": np.mean(densities_at_d),\n",
    "                \"q3\": np.percentile(densities_at_d, 75),\n",
    "                \"max\": np.max(densities_at_d)\n",
    "            }\n",
    "            density_stats_by_distance.append(stats)\n",
    "    \n",
    "    return density_stats_by_distance\n",
    "\n",
    "# Calculate enhanced density gradients\n",
    "density_stats = calculate_enhanced_density_gradients(urban_centers_mask, urban_mask, combined_image)\n",
    "\n",
    "# Extract data for plotting - ensure all arrays are created from the same source\n",
    "distances = np.array([stat[\"distance\"] for stat in density_stats])\n",
    "mean_densities = np.array([stat[\"mean\"] for stat in density_stats])\n",
    "median_densities = np.array([stat[\"median\"] for stat in density_stats])\n",
    "min_densities = np.array([stat[\"min\"] for stat in density_stats])\n",
    "max_densities = np.array([stat[\"max\"] for stat in density_stats])\n",
    "q1_densities = np.array([stat[\"q1\"] for stat in density_stats])\n",
    "q3_densities = np.array([stat[\"q3\"] for stat in density_stats])\n",
    "\n",
    "# Convert pixel distances to kilometers using satellite resolution\n",
    "distances_km = distances * satellite_pixel_resolution / 1000  # Divide by 1000 to convert m to km\n",
    "\n",
    "# Set gradient_intercept to the minimum of the Q1 values (minimum of IQR)\n",
    "gradient_intercept = np.min(q1_densities)\n",
    "\n",
    "# Find peaks and minima in the mean density profile\n",
    "if len(distances) > 5:\n",
    "    prominence = 0.02 * (np.max(mean_densities) - np.min(mean_densities))\n",
    "    peaks, _ = find_peaks(mean_densities, prominence=prominence)\n",
    "    \n",
    "    # Find local minima (valleys)\n",
    "    neg_densities = -1 * mean_densities\n",
    "    minima, _ = find_peaks(neg_densities, prominence=prominence)\n",
    "else:\n",
    "    peaks = []\n",
    "    minima = []\n",
    "\n",
    "# Skip the first point - don't add it to minima\n",
    "# Add only last point if needed\n",
    "if (len(distances)-1) not in peaks and (len(distances)-1) not in minima:\n",
    "    minima = np.append(minima, np.array([len(distances)-1]))\n",
    "\n",
    "# Sort minima to ensure they're in ascending order\n",
    "minima = np.sort(minima)\n",
    "\n",
    "# Calculate linear regression through minima\n",
    "if len(minima) > 1:\n",
    "    slope_pixel, intercept, r_value, p_value, std_err = scipy_stats.linregress(\n",
    "        distances[minima], mean_densities[minima])\n",
    "    \n",
    "    # Calculate slope in terms of km\n",
    "    slope_km = slope_pixel * 1000 / satellite_pixel_resolution  # Convert to per-km\n",
    "    \n",
    "    # Create trendline for all points\n",
    "    trendline = slope_pixel * distances + intercept\n",
    "    \n",
    "    # Calculate the residuals (mean density minus linear fitting)\n",
    "    residuals = mean_densities - trendline\n",
    "    \n",
    "    # Calculate x-value where the line intersects target_y_value\n",
    "    if abs(slope_pixel) > 1e-10:  # Avoid division by near-zero\n",
    "        x_pixels_at_target_y = (gradient_intercept - intercept) / slope_pixel\n",
    "        ld_distance_irq_km = x_pixels_at_target_y * satellite_pixel_resolution / 1000  # Convert to km\n",
    "    else:\n",
    "        x_pixels_at_target_y = float('inf')  # If slope is essentially zero\n",
    "        ld_distance_irq_km = float('inf')\n",
    "    \n",
    "    # Calculate MSE between actual and trendline\n",
    "    mse = mean_squared_error(mean_densities, trendline)\n",
    "else:\n",
    "    mse = 0\n",
    "    ld_distance_irq_km = float('inf')\n",
    "    slope_km = 0\n",
    "    intercept = 0\n",
    "    trendline = np.zeros_like(distances)\n",
    "    residuals = np.zeros_like(distances)\n",
    "\n",
    "# Create enhanced density gradient plot with two subplots (main plot and residuals)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "# Create GridSpec to have more control over spacing\n",
    "gs = fig.add_gridspec(2, 1, height_ratios=[3, 1])\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax2 = fig.add_subplot(gs[1], sharex=ax1)\n",
    "\n",
    "# MAIN PLOT (TOP) - Now including both density data and fit in the same plot\n",
    "# Plot density ranges on main plot\n",
    "ax1.fill_between(distances_km, min_densities, max_densities, alpha=0.1, color='blue', label='Min-Max Range')\n",
    "ax1.fill_between(distances_km, q1_densities, q3_densities, alpha=0.3, color='blue', label='IQR (25-75%)')\n",
    "\n",
    "# Plot average and median lines on main plot\n",
    "ax1.plot(distances_km, mean_densities, 'b-', linewidth=2, label='Mean Density')\n",
    "ax1.plot(distances_km, median_densities, 'g-', linewidth=1.5, label='Median Density')\n",
    "\n",
    "# Add the trendline on main plot\n",
    "if len(minima) > 1:\n",
    "    ax1.plot(distances_km, trendline, 'r--', linewidth=2, \n",
    "             label=f'Alfa: {slope_km:.4f}/km\\nDistance: {ld_distance_irq_km:.2f} km')\n",
    "\n",
    "# Add horizontal lines for thresholds on main plot\n",
    "ax1.axhline(y=urban_center_threshold, color='r', linestyle='--', alpha=0.7, \n",
    "           label=f\"Centers Threshold ({urban_center_threshold})\")\n",
    "ax1.axhline(y=urban_threshold, color='orange', linestyle='--', alpha=0.7, \n",
    "           label=f\"Urban Threshold ({urban_threshold})\")\n",
    "ax1.axhline(y=gradient_intercept, color='m', linestyle='--', alpha=0.7, \n",
    "           label=f\"IQR Min ({gradient_intercept:.2f})\")\n",
    "\n",
    "# Mark minima points used for regression on main plot\n",
    "ax1.plot(distances_km[minima], mean_densities[minima], 'rx', markersize=10)\n",
    "\n",
    "# If the intercept is within the plot range, mark it on main plot\n",
    "if 0 <= ld_distance_irq_km <= max(distances_km):\n",
    "    ax1.axvline(x=ld_distance_irq_km, color='m', linestyle=':', alpha=0.5)\n",
    "    ax1.plot(ld_distance_irq_km, gradient_intercept, 'mo', markersize=8)\n",
    "\n",
    "# Set title for main plot\n",
    "ax1.set_title(f'Enhanced Density Gradient (MSE: {mse:.6f}, Intercept: {gradient_intercept:.4f})')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "# RESIDUAL PLOT (BOTTOM)\n",
    "if len(minima) > 1:\n",
    "    # Create a filled area plot for residuals\n",
    "    ax2.plot(distances_km, residuals, 'r-', linewidth=2, label='Density - Fit')\n",
    "    ax2.fill_between(distances_km, 0, residuals, color='red', alpha=0.2)\n",
    "    \n",
    "    # Add horizontal line at y=0 for reference\n",
    "    ax2.axhline(y=0, color='k', linestyle='-', alpha=0.5)\n",
    "    \n",
    "    # Set labels for residual plot\n",
    "    ax2.set_ylabel('Density - Fit', color='red')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    \n",
    "    # Add grid to residual plot\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "# Set x-axis ticks using the defined interval (for both plots through sharex)\n",
    "max_distance_km = np.ceil(max(distances_km))\n",
    "x_ticks = np.arange(0, max_distance_km + 1, gradient_x_tick_interval/1000)  # Convert m to km\n",
    "ax2.set_xticks(x_ticks)\n",
    "ax2.set_xlabel('Distance from Urban Center (km)')\n",
    "\n",
    "# Hide x-axis labels on top plot\n",
    "plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "\n",
    "# Manually adjust the spacing instead of using tight_layout\n",
    "plt.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9, hspace=0.0)\n",
    "\n",
    "# Save and display the plot\n",
    "plt.savefig(f'{output_dir}/{base_name}_enhanced_density_gradient.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe1357-da68-4d36-8629-416681c779a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_residuals_limited(distances_km, residuals, min_regions=2, max_regions=4):\n",
    "    \"\"\"\n",
    "    Segment residuals into a limited number of regions (between min_regions and max_regions).\n",
    "    \n",
    "    Parameters:\n",
    "    - distances_km: Array of distance values in km\n",
    "    - residuals: Array of residual values (density - fit)\n",
    "    - min_regions: Minimum number of regions to identify\n",
    "    - max_regions: Maximum number of regions to identify\n",
    "    \n",
    "    Returns:\n",
    "    - regions: List of tuples with (start_index, end_index, type) for identified regions\n",
    "              where type is 'uniform' or 'variation'\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.signal import find_peaks\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    # Ensure we have enough data points\n",
    "    if len(residuals) < min_regions:\n",
    "        # If not enough data, return a single region\n",
    "        return [(0, len(residuals)-1, 'unknown')]\n",
    "    \n",
    "    # Calculate metrics for segmentation\n",
    "    abs_residuals = np.abs(residuals)\n",
    "    gradients = np.gradient(residuals)\n",
    "    abs_gradients = np.abs(gradients)\n",
    "    \n",
    "    # Prepare features for clustering\n",
    "    # We use both residual values and their gradients for better segmentation\n",
    "    features = np.column_stack((\n",
    "        residuals,\n",
    "        gradients,\n",
    "        np.roll(gradients, 1),  # Shifted gradients to capture transitions\n",
    "        np.roll(residuals, 1)   # Shifted residuals\n",
    "    ))\n",
    "    \n",
    "    # Replace NaN values (from rolling)\n",
    "    features[0, 2:] = features[1, 2:]\n",
    "    \n",
    "    # Try different numbers of clusters to find optimal segmentation\n",
    "    best_regions = []\n",
    "    best_score = -float('inf')\n",
    "    \n",
    "    for n_clusters in range(min_regions, max_regions + 1):\n",
    "        # Apply KMeans clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(features)\n",
    "        \n",
    "        # Convert cluster labels to contiguous regions\n",
    "        temp_regions = []\n",
    "        current_label = labels[0]\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i in range(1, len(labels)):\n",
    "            if labels[i] != current_label:\n",
    "                # End of a region\n",
    "                temp_regions.append((start_idx, i-1, current_label))\n",
    "                current_label = labels[i]\n",
    "                start_idx = i\n",
    "        \n",
    "        # Add the last region\n",
    "        temp_regions.append((start_idx, len(labels)-1, current_label))\n",
    "        \n",
    "        # Merge small regions (less than 5% of total) with adjacent regions\n",
    "        min_region_size = max(3, int(0.05 * len(residuals)))\n",
    "        i = 0\n",
    "        while i < len(temp_regions):\n",
    "            start, end, label = temp_regions[i]\n",
    "            if end - start + 1 < min_region_size:\n",
    "                # This is a small region that should be merged\n",
    "                if i > 0:\n",
    "                    # Merge with previous region\n",
    "                    prev_start, prev_end, prev_label = temp_regions[i-1]\n",
    "                    temp_regions[i-1] = (prev_start, end, prev_label)\n",
    "                    temp_regions.pop(i)\n",
    "                elif i < len(temp_regions) - 1:\n",
    "                    # Merge with next region\n",
    "                    next_start, next_end, next_label = temp_regions[i+1]\n",
    "                    temp_regions[i] = (start, next_end, next_label)\n",
    "                    temp_regions.pop(i+1)\n",
    "                else:\n",
    "                    # This is the only region, keep it\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        # Calculate a score for this segmentation based on:\n",
    "        # 1. Variance within each region (lower is better)\n",
    "        # 2. Number of regions close to desired (closer to max_regions is better)\n",
    "        within_variance = 0\n",
    "        for start, end, label in temp_regions:\n",
    "            region_residuals = residuals[start:end+1]\n",
    "            within_variance += np.var(region_residuals) * (end - start + 1)\n",
    "        \n",
    "        within_variance /= len(residuals)\n",
    "        \n",
    "        # Penalize having too few regions\n",
    "        region_count_score = -abs(len(temp_regions) - max_regions) if len(temp_regions) >= min_regions else -float('inf')\n",
    "        \n",
    "        # Total score (lower variance and more regions is better)\n",
    "        score = region_count_score - within_variance\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_regions = temp_regions\n",
    "    \n",
    "    # Classify each region as 'uniform' or 'variation'\n",
    "    classified_regions = []\n",
    "    \n",
    "    for start, end, label in best_regions:\n",
    "        region_residuals = residuals[start:end+1]\n",
    "        region_std = np.std(region_residuals)\n",
    "        \n",
    "        # Use the overall standard deviation as a reference\n",
    "        overall_std = np.std(residuals)\n",
    "        \n",
    "        if region_std < 0.7 * overall_std:\n",
    "            region_type = 'uniform'\n",
    "        else:\n",
    "            region_type = 'variation'\n",
    "        \n",
    "        classified_regions.append((start, end, region_type))\n",
    "    \n",
    "    return classified_regions\n",
    "\n",
    "def plot_segmented_residuals_limited(distances_km, residuals, regions):\n",
    "    \"\"\"\n",
    "    Plot the residuals with segmentation highlighted using background colors\n",
    "    and black dashed vertical lines for region boundaries.\n",
    "    \n",
    "    Parameters:\n",
    "    - distances_km: Array of distance values in km\n",
    "    - residuals: Array of residual values (density - fit)\n",
    "    - regions: List of tuples with (start_index, end_index, type) for identified regions\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from matplotlib.patches import Patch\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Define colors for region types with better contrast and transparency\n",
    "    region_colors = {\n",
    "        'uniform': 'lightgreen',\n",
    "        'variation': 'lightsalmon',\n",
    "        'unknown': 'lightgray'\n",
    "    }\n",
    "    \n",
    "    # Ensure regions are ordered by start index\n",
    "    sorted_regions = sorted(regions, key=lambda x: x[0])\n",
    "    \n",
    "    # First, add background colors for all regions and collect boundary points\n",
    "    boundary_indices = []\n",
    "    \n",
    "    for i, (start, end, region_type) in enumerate(sorted_regions):\n",
    "        # Add indices to boundary points\n",
    "        if i == 0:  # First region\n",
    "            boundary_indices.append(start)\n",
    "        boundary_indices.append(end)\n",
    "        \n",
    "        # Color the background for this region\n",
    "        color = region_colors.get(region_type)\n",
    "        if i < len(sorted_regions) - 1:\n",
    "            # For all regions except the last, go from start to the next region's start\n",
    "            next_start = sorted_regions[i+1][0]\n",
    "            x_start = distances_km[start]\n",
    "            x_end = distances_km[next_start]\n",
    "        else:\n",
    "            # For the last region\n",
    "            x_start = distances_km[start]\n",
    "            x_end = distances_km[end]\n",
    "        \n",
    "        ax.axvspan(x_start, x_end, color=color, alpha=0.3, zorder=0)\n",
    "    \n",
    "    # Plot the residuals line on top of the background\n",
    "    ax.plot(distances_km, residuals, 'k-', linewidth=1.5, label='Residuals (Density - Fit)', zorder=3)\n",
    "    \n",
    "    # Fill the area under the residuals curve with semi-transparent gray\n",
    "    ax.fill_between(distances_km, 0, residuals, color='gray', alpha=0.2, zorder=2)\n",
    "    \n",
    "    # Add a horizontal line at zero\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.5, zorder=1)\n",
    "    \n",
    "    # Get y limits for vertical lines\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    \n",
    "    # Mark only unique boundary points on the x-axis with vertical dashed lines\n",
    "    for i, idx in enumerate(boundary_indices):\n",
    "        x = distances_km[idx]\n",
    "        \n",
    "        # Add vertical dashed line from bottom to top\n",
    "        ax.axvline(x=x, color='black', linestyle='--', linewidth=1, alpha=0.7, zorder=1)\n",
    "        \n",
    "        # Mark boundary on x-axis with a triangle\n",
    "        ax.plot(x, 0, 'o', color='red', markersize=8, markeredgewidth=1, \n",
    "               markeredgecolor='white', zorder=4)\n",
    "        \n",
    "        # Add distance label at boundary\n",
    "        ax.text(x, 0, f' {x:.2f}', verticalalignment='bottom', \n",
    "               horizontalalignment='left', fontsize=10, rotation=45, color='red')\n",
    "    \n",
    "    # Add region labels in the middle of each region\n",
    "    for i, (start, end, region_type) in enumerate(sorted_regions):\n",
    "        # For label position, use the midpoint between this region's start and the next region's start\n",
    "        if i < len(sorted_regions) - 1:\n",
    "            next_start = sorted_regions[i+1][0]\n",
    "            region_mid = (distances_km[start] + distances_km[next_start]) / 2\n",
    "        else:\n",
    "            region_mid = (distances_km[start] + distances_km[end]) / 2\n",
    "            \n",
    "        if region_type == 'uniform':\n",
    "            text_color = 'darkgreen'\n",
    "        else:\n",
    "            text_color = 'darkred'\n",
    "        ax.text(region_mid, 0, f'R{i+1}', verticalalignment='bottom', \n",
    "               horizontalalignment='center', fontsize=10, fontweight='bold', \n",
    "               color=text_color, bbox=dict(facecolor='white', alpha=0.7, pad=2))\n",
    "    \n",
    "    # Create a custom legend with region information\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=region_colors['uniform'], alpha=0.3, edgecolor='green', \n",
    "              label='Uniform Regions'),\n",
    "        Patch(facecolor=region_colors['variation'], alpha=0.3, edgecolor='red', \n",
    "              label='High-Variation Regions')\n",
    "    ]\n",
    "    \n",
    "    # Add the residuals line to the legend elements\n",
    "    legend_elements.append(plt.Line2D([0], [0], color='black', linewidth=1.5, \n",
    "                                    label='Residuals (Density - Fit)'))\n",
    "    \n",
    "    # Create a separate text box with region details\n",
    "    region_info = []\n",
    "    for i, (start, end, region_type) in enumerate(sorted_regions):\n",
    "        # For reporting region extents, use the next region's start as the end boundary\n",
    "        if i < len(sorted_regions) - 1:\n",
    "            next_start = sorted_regions[i+1][0]\n",
    "            x_start = distances_km[start]\n",
    "            x_end = distances_km[next_start]\n",
    "        else:\n",
    "            x_start = distances_km[start]\n",
    "            x_end = distances_km[end]\n",
    "            \n",
    "        if region_type == 'uniform':\n",
    "            color_name = 'green'\n",
    "        else:\n",
    "            color_name = 'red'\n",
    "        region_info.append(f\"Region {i+1} ({x_start:.2f} - {x_end:.2f} km): {region_type.capitalize()}\")\n",
    "    \n",
    "    # Add the region information text box\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
    "    ax.text(0.02, 0.98, '\\n'.join(region_info), transform=ax.transAxes, fontsize=9,\n",
    "           verticalalignment='top', bbox=props)\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('Distance from Urban Center (km)')\n",
    "    ax.set_ylabel('Density - Fit')\n",
    "    ax.set_title('Segmentation of Residuals')\n",
    "    ax.grid(True, alpha=0.3, zorder=0)\n",
    "    \n",
    "    # Add the main legend\n",
    "    ax.legend(handles=legend_elements, loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def segment_residuals_limited(distances_km, residuals, min_regions=2, max_regions=4):\n",
    "    \"\"\"\n",
    "    Segment residuals into a limited number of regions (between min_regions and max_regions).\n",
    "    Ensures that region boundaries match exactly.\n",
    "    \n",
    "    Parameters:\n",
    "    - distances_km: Array of distance values in km\n",
    "    - residuals: Array of residual values (density - fit)\n",
    "    - min_regions: Minimum number of regions to identify\n",
    "    - max_regions: Maximum number of regions to identify\n",
    "    \n",
    "    Returns:\n",
    "    - regions: List of tuples with (start_index, end_index, type) for identified regions\n",
    "              where type is 'uniform' or 'variation'\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.signal import find_peaks\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    # Ensure we have enough data points\n",
    "    if len(residuals) < min_regions:\n",
    "        # If not enough data, return a single region\n",
    "        return [(0, len(residuals)-1, 'unknown')]\n",
    "    \n",
    "    # Calculate metrics for segmentation\n",
    "    abs_residuals = np.abs(residuals)\n",
    "    gradients = np.gradient(residuals)\n",
    "    abs_gradients = np.abs(gradients)\n",
    "    \n",
    "    # Prepare features for clustering\n",
    "    # We use both residual values and their gradients for better segmentation\n",
    "    features = np.column_stack((\n",
    "        residuals,\n",
    "        gradients,\n",
    "        np.roll(gradients, 1),  # Shifted gradients to capture transitions\n",
    "        np.roll(residuals, 1)   # Shifted residuals\n",
    "    ))\n",
    "    \n",
    "    # Replace NaN values (from rolling)\n",
    "    features[0, 2:] = features[1, 2:]\n",
    "    \n",
    "    # Try different numbers of clusters to find optimal segmentation\n",
    "    best_regions = []\n",
    "    best_score = -float('inf')\n",
    "    \n",
    "    for n_clusters in range(min_regions, max_regions + 1):\n",
    "        # Apply KMeans clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(features)\n",
    "        \n",
    "        # Convert cluster labels to contiguous regions\n",
    "        temp_regions = []\n",
    "        current_label = labels[0]\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i in range(1, len(labels)):\n",
    "            if labels[i] != current_label:\n",
    "                # End of a region\n",
    "                temp_regions.append((start_idx, i-1, current_label))\n",
    "                current_label = labels[i]\n",
    "                start_idx = i\n",
    "        \n",
    "        # Add the last region\n",
    "        temp_regions.append((start_idx, len(labels)-1, current_label))\n",
    "        \n",
    "        # Merge small regions (less than 5% of total) with adjacent regions\n",
    "        min_region_size = max(3, int(0.05 * len(residuals)))\n",
    "        i = 0\n",
    "        while i < len(temp_regions):\n",
    "            start, end, label = temp_regions[i]\n",
    "            if end - start + 1 < min_region_size:\n",
    "                # This is a small region that should be merged\n",
    "                if i > 0:\n",
    "                    # Merge with previous region\n",
    "                    prev_start, prev_end, prev_label = temp_regions[i-1]\n",
    "                    temp_regions[i-1] = (prev_start, end, prev_label)\n",
    "                    temp_regions.pop(i)\n",
    "                elif i < len(temp_regions) - 1:\n",
    "                    # Merge with next region\n",
    "                    next_start, next_end, next_label = temp_regions[i+1]\n",
    "                    temp_regions[i] = (start, next_end, next_label)\n",
    "                    temp_regions.pop(i+1)\n",
    "                else:\n",
    "                    # This is the only region, keep it\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        # Calculate a score for this segmentation based on:\n",
    "        # 1. Variance within each region (lower is better)\n",
    "        # 2. Number of regions close to desired (closer to max_regions is better)\n",
    "        within_variance = 0\n",
    "        for start, end, label in temp_regions:\n",
    "            region_residuals = residuals[start:end+1]\n",
    "            within_variance += np.var(region_residuals) * (end - start + 1)\n",
    "        \n",
    "        within_variance /= len(residuals)\n",
    "        \n",
    "        # Penalize having too few regions\n",
    "        region_count_score = -abs(len(temp_regions) - max_regions) if len(temp_regions) >= min_regions else -float('inf')\n",
    "        \n",
    "        # Total score (lower variance and more regions is better)\n",
    "        score = region_count_score - within_variance\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_regions = temp_regions\n",
    "    \n",
    "    # Sort regions by start index to ensure they're in order\n",
    "    best_regions.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Ensure the first region starts at index 0\n",
    "    if best_regions[0][0] > 0:\n",
    "        best_regions[0] = (0, best_regions[0][1], best_regions[0][2])\n",
    "    \n",
    "    # Ensure the last region ends at the last data point\n",
    "    if best_regions[-1][1] < len(residuals) - 1:\n",
    "        best_regions[-1] = (best_regions[-1][0], len(residuals) - 1, best_regions[-1][2])\n",
    "    \n",
    "    # Create perfectly contiguous regions\n",
    "    contiguous_regions = []\n",
    "    for i, (start, end, label) in enumerate(best_regions):\n",
    "        if i == 0:\n",
    "            # First region starts at 0\n",
    "            contiguous_regions.append((0, end, label))\n",
    "        else:\n",
    "            # Each subsequent region starts exactly where the previous one ended\n",
    "            prev_end = contiguous_regions[-1][1]\n",
    "            contiguous_regions.append((prev_end + 1, end, label))\n",
    "    \n",
    "    # Make sure there are no gaps between regions\n",
    "    for i in range(1, len(contiguous_regions)):\n",
    "        prev_end = contiguous_regions[i-1][1]\n",
    "        current_start = contiguous_regions[i][0]\n",
    "        \n",
    "        if current_start != prev_end + 1:\n",
    "            # Fix the gap\n",
    "            contiguous_regions[i] = (prev_end + 1, contiguous_regions[i][1], contiguous_regions[i][2])\n",
    "    \n",
    "    # Classify each region as 'uniform' or 'variation'\n",
    "    classified_regions = []\n",
    "    \n",
    "    for start, end, label in contiguous_regions:\n",
    "        region_residuals = residuals[start:end+1]\n",
    "        region_std = np.std(region_residuals)\n",
    "        \n",
    "        # Use the overall standard deviation as a reference\n",
    "        overall_std = np.std(residuals)\n",
    "        \n",
    "        if region_std < 0.7 * overall_std:\n",
    "            region_type = 'uniform'\n",
    "        else:\n",
    "            region_type = 'variation'\n",
    "        \n",
    "        classified_regions.append((start, end, region_type))\n",
    "    \n",
    "    return classified_regions\n",
    "\n",
    "def analyze_segmented_residuals_limited(distances_km, residuals, regions):\n",
    "    \"\"\"\n",
    "    Analyze the segmented residuals and print detailed statistics for the limited regions.\n",
    "    \n",
    "    Parameters:\n",
    "    - distances_km: Array of distance values in km\n",
    "    - residuals: Array of residual values (density - fit)\n",
    "    - regions: List of tuples with (start_index, end_index, type) for identified regions\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    \n",
    "    print(\"=== RESIDUALS SEGMENTATION ANALYSIS (LIMITED REGIONS) ===\")\n",
    "    \n",
    "    # Overall stats\n",
    "    print(\"\\nOVERALL STATISTICS:\")\n",
    "    print(f\"Total distance analyzed: {distances_km[-1] - distances_km[0]:.2f} km\")\n",
    "    print(f\"Number of data points: {len(residuals)}\")\n",
    "    print(f\"Mean residual: {np.mean(residuals):.4f}\")\n",
    "    print(f\"Standard deviation: {np.std(residuals):.4f}\")\n",
    "    print(f\"Range: {np.max(residuals) - np.min(residuals):.4f}\")\n",
    "    \n",
    "    # Region analysis\n",
    "    print(f\"\\nIDENTIFIED REGIONS: {len(regions)}\")\n",
    "    \n",
    "    for i, (start, end, region_type) in enumerate(regions):\n",
    "        region_distances = distances_km[start:end+1]\n",
    "        region_residuals = residuals[start:end+1]\n",
    "        region_length = region_distances[-1] - region_distances[0]\n",
    "        \n",
    "        print(f\"\\nRegion {i+1} ({region_type.upper()}):\")\n",
    "        print(f\"  Distance range: {region_distances[0]:.2f} km to {region_distances[-1]:.2f} km (Length: {region_length:.2f} km)\")\n",
    "        print(f\"  Mean: {np.mean(region_residuals):.4f}\")\n",
    "        print(f\"  Standard deviation: {np.std(region_residuals):.4f}\")\n",
    "        print(f\"  Range: {np.max(region_residuals) - np.min(region_residuals):.4f}\")\n",
    "        \n",
    "        if region_type == 'variation':\n",
    "            # Calculate peaks\n",
    "            from scipy.signal import find_peaks\n",
    "            peaks, _ = find_peaks(region_residuals, prominence=0.01)\n",
    "            valleys, _ = find_peaks(-region_residuals, prominence=0.01)\n",
    "            \n",
    "            if len(peaks) > 0:\n",
    "                peak_distances = [region_distances[p] for p in peaks]\n",
    "                peak_values = [region_residuals[p] for p in peaks]\n",
    "                print(f\"  Peaks: {len(peaks)} found at distances: {', '.join([f'{d:.2f} km' for d in peak_distances])}\")\n",
    "                print(f\"  Peak values: {', '.join([f'{v:.4f}' for v in peak_values])}\")\n",
    "            \n",
    "            if len(valleys) > 0:\n",
    "                valley_distances = [region_distances[v] for v in valleys]\n",
    "                valley_values = [region_residuals[v] for v in valleys]\n",
    "                print(f\"  Valleys: {len(valleys)} found at distances: {', '.join([f'{d:.2f} km' for d in valley_distances])}\")\n",
    "                print(f\"  Valley values: {', '.join([f'{v:.4f}' for v in valley_values])}\")\n",
    "    \n",
    "    print(\"\\n=== END OF ANALYSIS ===\")\n",
    "\n",
    "# Using the new limited segmentation approach\n",
    "# Segment the residuals with limited regions\n",
    "regions = segment_residuals_limited(\n",
    "    distances_km, \n",
    "    residuals, \n",
    "    min_regions=1,    # Minimum number of regions to identify\n",
    "    max_regions=3     # Maximum number of regions to identify\n",
    ")\n",
    "\n",
    "# Plot the segmented residuals\n",
    "fig = plot_segmented_residuals_limited(distances_km, residuals, regions)\n",
    "\n",
    "# Analyze and print detailed statistics\n",
    "analyze_segmented_residuals_limited(distances_km, residuals, regions)\n",
    "\n",
    "# Save the segmentation plot\n",
    "output_segmentation_path = f'{output_dir}/{base_name}_residual_segmentation_limited.png'\n",
    "fig.savefig(output_segmentation_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Segmentation plot saved to: {output_segmentation_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa2b397-60cd-45d9-b65e-ee78543ad875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define distance for circle radius\n",
    "ld_distance_km = ld_distance_irq_km / 2\n",
    "\n",
    "# Define center sizse selection for plots\n",
    "center_min_size = 000\n",
    "center_max_size = 5\n",
    "\n",
    "# Find centers of urban center clusters\n",
    "num_centers, center_labels, center_stats, center_centroids = cv2.connectedComponentsWithStats(\n",
    "    urban_centers_mask.astype(np.uint8), connectivity=8)\n",
    "\n",
    "# Convert combined image to a grayscale background\n",
    "# Normalize to 0-255 range\n",
    "background_image = np.clip(combined_image * 255 / combined_image.max(), 0, 255).astype(np.uint8)\n",
    "# Convert to 3-channel grayscale image for colored overlays\n",
    "background_image_rgb = cv2.cvtColor(background_image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# Create a copy for drawing\n",
    "result_image = background_image_rgb.copy()\n",
    "\n",
    "# Create a separate mask for all circles\n",
    "circles_mask = np.zeros_like(result_image)\n",
    "radius_pixels = int(ld_distance_km * 1000 / satellite_pixel_resolution)  # Convert km to pixels\n",
    "\n",
    "# Define red color in RGB format for consistency with matplotlib\n",
    "red_color = (255, 0, 0)  # This is RED in RGB format\n",
    "\n",
    "# Draw all circles on the circles mask\n",
    "for i in range(1, num_centers):\n",
    "    # Skip clusters not in the desired size range\n",
    "    if (center_stats[i, cv2.CC_STAT_AREA] < center_min_size) or (center_stats[i, cv2.CC_STAT_AREA] > center_max_size):\n",
    "        continue\n",
    "        \n",
    "    # Get center coordinates and convert to integers\n",
    "    center_y, center_x = center_centroids[i]\n",
    "    center = (int(center_x), int(center_y))\n",
    "    \n",
    "    # Draw a filled circle on the circles mask (solid red)\n",
    "    cv2.circle(circles_mask, center, radius_pixels, red_color, -1)  # Red filled circle\n",
    "    \n",
    "    # Draw circle boundary in red (same color as fill)\n",
    "    cv2.circle(circles_mask, center, radius_pixels, red_color, 2)  # Red border\n",
    "\n",
    "    break\n",
    "\n",
    "# Now combine the background and circles with proper alpha\n",
    "# Create binary mask where circles exist\n",
    "binary_mask = (circles_mask > 0).any(axis=2)\n",
    "# Apply the circles with alpha=0.2 only where circles exist\n",
    "alpha = 0.2\n",
    "result_image[binary_mask] = cv2.addWeighted(\n",
    "    circles_mask[binary_mask], \n",
    "    alpha, \n",
    "    result_image[binary_mask], \n",
    "    1 - alpha, \n",
    "    0\n",
    ")\n",
    "\n",
    "# Convert to BGR for saving with cv2.imwrite\n",
    "result_image_bgr = cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR)\n",
    "cv2.imwrite(f'data/{base_name}_urban_centers_circles_gray.png', result_image_bgr)\n",
    "\n",
    "# Display using matplotlib (which expects RGB)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(result_image)\n",
    "set_axes_in_meters(plt.gca(), result_image.shape)\n",
    "plt.title(f'Urban Centers with {ld_distance_km:.2f} km Radius Circles')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2663ce3f-77e5-4e1b-b0d1-106c2173ae5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
